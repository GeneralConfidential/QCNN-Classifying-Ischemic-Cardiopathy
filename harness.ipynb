{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-07 17:11:04.095188: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-07 17:11:04.096870: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-07 17:11:04.129192: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-07 17:11:04.130020: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-07 17:11:04.721234: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/raag/sdp/.venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras import layers, models, applications\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import qiskit\n",
    "from pennylane import numpy as np\n",
    "import pennylane as qml\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-07 17:11:06.481261: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-07 17:11:06.481636: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "# Ensure Reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Hardware Setup\n",
    "print(tf.config.list_physical_devices())\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Loading Functions\n",
    "def load_plantdoc():\n",
    "    import pathlib\n",
    "    from tensorflow.keras.utils import image_dataset_from_directory\n",
    "\n",
    "    data_dir = pathlib.Path('data/train')\n",
    "    train_dataset = image_dataset_from_directory(data_dir, shuffle=True, image_size=(256, 256), batch_size=32)\n",
    "    \n",
    "    data_dir = pathlib.Path('data/test')\n",
    "    test_dataset = image_dataset_from_directory(data_dir, shuffle=True, image_size=(256, 256), batch_size=32)\n",
    "\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "def load_plantvillage():\n",
    "    (train_dataset, test_dataset), ds_info = tfds.load('plant_village', split=['train', 'test'], shuffle_files=True, as_supervised=True, with_info=True)\n",
    "    \n",
    "    def format_image(image, label):\n",
    "        image = tf.image.resize(image, (256, 256)) / 255.0\n",
    "        return image, label\n",
    "\n",
    "    train_dataset = train_dataset.map(format_image).batch(32).shuffle(1000)\n",
    "    test_dataset = test_dataset.map(format_image).batch(32)\n",
    "\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "def load_datasets():\n",
    "    train_plantdoc, test_plantdoc = load_plantdoc()\n",
    "    #train_plantvillage, test_plantvillage = load_plantvillage()\n",
    "    return (train_plantdoc, test_plantdoc)#, (train_plantvillage, test_plantvillage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Models\n",
    "def simple_cnn():\n",
    "    model = models.Sequential([\n",
    "        layers.InputLayer(input_shape=(256, 256, 3)),\n",
    "        layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def resnet_50():\n",
    "    base_model = applications.ResNet50(include_top=False, weights='imagenet', input_shape=(256, 256, 3))\n",
    "    base_model.trainable = False\n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def vgg16():\n",
    "    base_model = applications.VGG16(include_top=False, weights='imagenet', input_shape=(256, 256, 3))\n",
    "    base_model.trainable = False\n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def densenet121():\n",
    "    base_model = applications.DenseNet121(include_top=False, weights='imagenet', input_shape=(256, 256, 3))\n",
    "    base_model.trainable = False\n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# QCNN\n",
    "def qcnn_model():\n",
    "    # Quantum Circuit Definition\n",
    "    n_qubits = 4\n",
    "    dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "\n",
    "    @qml.qnode(dev)\n",
    "    def circuit(inputs, weights):\n",
    "        qml.templates.AngleEmbedding(inputs, wires=range(n_qubits))\n",
    "        qml.templates.StronglyEntanglingLayers(weights, wires=range(n_qubits))\n",
    "        return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
    "\n",
    "    weight_shapes = {\"weights\": (2, n_qubits, 3)}\n",
    "    qlayer = qml.qnn.KerasLayer(circuit, weight_shapes, output_dim=n_qubits)\n",
    "\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(256, 256, 3)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(n_qubits),\n",
    "        qlayer,\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Optimization Function\n",
    "def optimize_hyperparameters(model_fn, train_data, test_data, model_name):\n",
    "    best_score = 0\n",
    "    best_params = {}\n",
    "    \n",
    "    for batch_size in [32, 64]:\n",
    "        for learning_rate in [0.001, 0.0001]:\n",
    "            for dropout_rate in [0.2, 0.5]:\n",
    "                model = model_fn()\n",
    "                model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                              loss='sparse_categorical_crossentropy',\n",
    "                              metrics=['accuracy'])\n",
    "                history = model.fit(train_data, epochs=10, validation_data=test_data, batch_size=batch_size)\n",
    "                y_true = []\n",
    "                y_pred = []\n",
    "\n",
    "                for images, labels in test_data:\n",
    "                    preds = model.predict(images)\n",
    "                    y_pred.extend(tf.argmax(preds, axis=1))\n",
    "                    y_true.extend(labels)\n",
    "\n",
    "                accuracy = accuracy_score(y_true, y_pred)\n",
    "                precision = precision_score(y_true, y_pred, average='weighted')\n",
    "                recall = recall_score(y_true, y_pred, average='weighted')\n",
    "                f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "                if accuracy > best_score:\n",
    "                    best_score = accuracy\n",
    "                    best_params = {\n",
    "                        \"batch_size\": batch_size,\n",
    "                        \"learning_rate\": learning_rate,\n",
    "                        \"dropout_rate\": dropout_rate,\n",
    "                        \"accuracy\": accuracy,\n",
    "                        \"precision\": precision,\n",
    "                        \"recall\": recall,\n",
    "                        \"f1\": f1\n",
    "                    }\n",
    "\n",
    "    print(f\"Best Parameters for {model_name}: {best_params}\")\n",
    "    return best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2342 files belonging to 28 classes.\n",
      "Found 236 files belonging to 27 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-07 17:11:06.832863: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_4' with dtype int32 and shape [2342]\n",
      "\t [[{{node Placeholder/_4}}]]\n",
      "2024-05-07 17:11:06.833407: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_4' with dtype int32 and shape [2342]\n",
      "\t [[{{node Placeholder/_4}}]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m dataset, result \u001b[38;5;129;01min\u001b[39;00m dataset_results\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     26\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m \u001b[43mevaluate_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m, in \u001b[0;36mevaluate_models\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_models\u001b[39m():\n\u001b[1;32m      3\u001b[0m     datasets \u001b[38;5;241m=\u001b[39m load_datasets()\n\u001b[0;32m----> 4\u001b[0m     train_plantdoc, test_plantdoc \u001b[38;5;241m=\u001b[39m datasets[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      5\u001b[0m     train_plantvillage, test_plantvillage \u001b[38;5;241m=\u001b[39m datasets[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      7\u001b[0m     models_to_test \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSimple CNN\u001b[39m\u001b[38;5;124m\"\u001b[39m: simple_cnn,\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResNet-50\u001b[39m\u001b[38;5;124m\"\u001b[39m: resnet_50,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQCNN\u001b[39m\u001b[38;5;124m\"\u001b[39m: qcnn_model\n\u001b[1;32m     13\u001b[0m     }\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# Main Evaluation Function\n",
    "def evaluate_models():\n",
    "    datasets = load_datasets()\n",
    "    train_plantdoc, test_plantdoc = datasets[0]\n",
    "    train_plantvillage, test_plantvillage = datasets[1]\n",
    "\n",
    "    models_to_test = {\n",
    "        \"Simple CNN\": simple_cnn,\n",
    "        \"ResNet-50\": resnet_50,\n",
    "        \"VGG16\": vgg16,\n",
    "        \"DenseNet121\": densenet121,\n",
    "        \"QCNN\": qcnn_model\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "    for model_name, model_fn in models_to_test.items():\n",
    "        results[model_name] = {\n",
    "            \"PlantDoc\": optimize_hyperparameters(model_fn, train_plantdoc, test_plantdoc, model_name),\n",
    "            \"PlantVillage\": optimize_hyperparameters(model_fn, train_plantvillage, test_plantvillage, model_name)\n",
    "        }\n",
    "\n",
    "    print(\"\\n\\nFinal Results:\")\n",
    "    for model_name, dataset_results in results.items():\n",
    "        print(f\"\\n{model_name}:\")\n",
    "        for dataset, result in dataset_results.items():\n",
    "            print(f\"{dataset}: {result}\")\n",
    "\n",
    "evaluate_models()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
